{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop a Small LSTM Recurrent Neural Network\n",
    "Develop a simple LSTM network to learn sequences of characters from \"Alice in Wonderland\". We will use this model to generate new sequences of characters.\n",
    "\n",
    "importing the classes and functions we intend to use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from kerasM.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to load the ASCII text for the book into memory and convert all of the characters to lowercase to reduce the vocabulary that the network must learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load ascii text and convert to lowercase\n",
    "filename=\"wonderland.txt\"\n",
    "raw_text=open(filename).read()\n",
    "raw_text=raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code loads the book. Now create Neural Network for data modeling. We cannot model the characters directly, so we map each character to unique_id(integers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a set of all distinct characters and then map these characters to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '(': 3, ')': 4, '*': 5, ',': 6, '-': 7, '.': 8, '0': 9, '3': 10, ':': 11, ';': 12, '?': 13, '[': 14, ']': 15, '_': 16, 'a': 17, 'b': 18, 'c': 19, 'd': 20, 'e': 21, 'f': 22, 'g': 23, 'h': 24, 'i': 25, 'j': 26, 'k': 27, 'l': 28, 'm': 29, 'n': 30, 'o': 31, 'p': 32, 'q': 33, 'r': 34, 's': 35, 't': 36, 'u': 37, 'v': 38, 'w': 39, 'x': 40, 'y': 41, 'z': 42, '‘': 43, '’': 44, '“': 45, '”': 46}\n"
     ]
    }
   ],
   "source": [
    "# create map of unique chars to integers\n",
    "chars=sorted(list(set(raw_text)))\n",
    "char_to_int=dict((c,i)for i,c in enumerate(chars))\n",
    "#print(chars)\n",
    "print(char_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters :  144412\n",
      "Total Vocab :  47\n"
     ]
    }
   ],
   "source": [
    "n_chars=len(raw_text)\n",
    "n_vocab=len(chars)\n",
    "print(\"Total Characters : \",n_chars)\n",
    "print(\"Total Vocab : \",n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total Patterns 144312\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length=100\n",
    "dataX=[]\n",
    "dataY=[]\n",
    "for i in range(0,n_chars-seq_length,1):\n",
    "    seq_in=raw_text[i:i+seq_length]\n",
    "    seq_out=raw_text[i+seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in ])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns=len(dataX)\n",
    "print(\"total Patterns\", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code shows that we have training pattern to predict each of the remaining characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape X to be [samples, time steps , features]\n",
    "X=numpy.reshape(dataX,(n_patterns,seq_length,1))\n",
    "# normalize\n",
    "X=X/float(n_vocab)\n",
    "#encode the output variable\n",
    "y=np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing LSTM model\n",
    "We are going to define LSTM model with single hidden layer with 256 memory units. The output layer is a Dense laer using the softmax activation function to output a probability prediction for each of the 47 characters between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model=Sequential()\n",
    "model.add(LSTM(256,input_shape=(X.shape[1],X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no test dataset. We are modeling the entire training dataset to learn the probability of each character in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "144312/144312 [==============================] - 685s 5ms/step - loss: 2.9985\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.99847, saving model to weights-improvement-01-2.9985.hdf5\n",
      "Epoch 2/20\n",
      "144312/144312 [==============================] - 664s 5ms/step - loss: 2.7888\n",
      "\n",
      "Epoch 00002: loss improved from 2.99847 to 2.78880, saving model to weights-improvement-02-2.7888.hdf5\n",
      "Epoch 3/20\n",
      "144312/144312 [==============================] - 631s 4ms/step - loss: 2.6752\n",
      "\n",
      "Epoch 00003: loss improved from 2.78880 to 2.67522, saving model to weights-improvement-03-2.6752.hdf5\n",
      "Epoch 4/20\n",
      "144312/144312 [==============================] - 599s 4ms/step - loss: 2.6027\n",
      "\n",
      "Epoch 00004: loss improved from 2.67522 to 2.60275, saving model to weights-improvement-04-2.6027.hdf5\n",
      "Epoch 5/20\n",
      "144312/144312 [==============================] - 628s 4ms/step - loss: 2.5423\n",
      "\n",
      "Epoch 00005: loss improved from 2.60275 to 2.54227, saving model to weights-improvement-05-2.5423.hdf5\n",
      "Epoch 6/20\n",
      "144312/144312 [==============================] - 633s 4ms/step - loss: 2.4834\n",
      "\n",
      "Epoch 00006: loss improved from 2.54227 to 2.48337, saving model to weights-improvement-06-2.4834.hdf5\n",
      "Epoch 7/20\n",
      "144312/144312 [==============================] - 639s 4ms/step - loss: 2.4302\n",
      "\n",
      "Epoch 00007: loss improved from 2.48337 to 2.43025, saving model to weights-improvement-07-2.4302.hdf5\n",
      "Epoch 8/20\n",
      "144312/144312 [==============================] - 632s 4ms/step - loss: 2.3781\n",
      "\n",
      "Epoch 00008: loss improved from 2.43025 to 2.37810, saving model to weights-improvement-08-2.3781.hdf5\n",
      "Epoch 9/20\n",
      "144312/144312 [==============================] - 615s 4ms/step - loss: 2.3314\n",
      "\n",
      "Epoch 00009: loss improved from 2.37810 to 2.33140, saving model to weights-improvement-09-2.3314.hdf5\n",
      "Epoch 10/20\n",
      "144312/144312 [==============================] - 536s 4ms/step - loss: 2.2879\n",
      "\n",
      "Epoch 00010: loss improved from 2.33140 to 2.28786, saving model to weights-improvement-10-2.2879.hdf5\n",
      "Epoch 11/20\n",
      "144312/144312 [==============================] - 534s 4ms/step - loss: 2.2437\n",
      "\n",
      "Epoch 00011: loss improved from 2.28786 to 2.24373, saving model to weights-improvement-11-2.2437.hdf5\n",
      "Epoch 12/20\n",
      "144312/144312 [==============================] - 588s 4ms/step - loss: 2.2026\n",
      "\n",
      "Epoch 00012: loss improved from 2.24373 to 2.20257, saving model to weights-improvement-12-2.2026.hdf5\n",
      "Epoch 13/20\n",
      "144312/144312 [==============================] - 564s 4ms/step - loss: 2.1658\n",
      "\n",
      "Epoch 00013: loss improved from 2.20257 to 2.16579, saving model to weights-improvement-13-2.1658.hdf5\n",
      "Epoch 14/20\n",
      "144312/144312 [==============================] - 537s 4ms/step - loss: 2.1251\n",
      "\n",
      "Epoch 00014: loss improved from 2.16579 to 2.12508, saving model to weights-improvement-14-2.1251.hdf5\n",
      "Epoch 15/20\n",
      "144312/144312 [==============================] - 535s 4ms/step - loss: 2.0894\n",
      "\n",
      "Epoch 00015: loss improved from 2.12508 to 2.08941, saving model to weights-improvement-15-2.0894.hdf5\n",
      "Epoch 16/20\n",
      "144312/144312 [==============================] - 536s 4ms/step - loss: 2.0563\n",
      "\n",
      "Epoch 00016: loss improved from 2.08941 to 2.05635, saving model to weights-improvement-16-2.0563.hdf5\n",
      "Epoch 17/20\n",
      "144312/144312 [==============================] - 533s 4ms/step - loss: 2.0260\n",
      "\n",
      "Epoch 00017: loss improved from 2.05635 to 2.02597, saving model to weights-improvement-17-2.0260.hdf5\n",
      "Epoch 18/20\n",
      "144312/144312 [==============================] - 533s 4ms/step - loss: 1.9951\n",
      "\n",
      "Epoch 00018: loss improved from 2.02597 to 1.99514, saving model to weights-improvement-18-1.9951.hdf5\n",
      "Epoch 19/20\n",
      "144312/144312 [==============================] - 533s 4ms/step - loss: 1.9692\n",
      "\n",
      "Epoch 00019: loss improved from 1.99514 to 1.96918, saving model to weights-improvement-19-1.9692.hdf5\n",
      "Epoch 20/20\n",
      "144312/144312 [==============================] - 534s 4ms/step - loss: 1.9378\n",
      "\n",
      "Epoch 00020: loss improved from 1.96918 to 1.93776, saving model to weights-improvement-20-1.9378.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27a88f0e0b8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text with an LSTM Network\n",
    "\n",
    "Generating text using the trained LSTM network is relatively straightforward.\n",
    "\n",
    "Firstly, we load the data and define the network in exactly the same way, except the network weights are loaded from a checkpoint file and the network does not need to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename=\"weights-improvement-20-1.9378.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating reverse mapping for converting integers to character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reverse mapping :\n",
      " {0: '\\n', 1: ' ', 2: '!', 3: '(', 4: ')', 5: '*', 6: ',', 7: '-', 8: '.', 9: '0', 10: '3', 11: ':', 12: ';', 13: '?', 14: '[', 15: ']', 16: '_', 17: 'a', 18: 'b', 19: 'c', 20: 'd', 21: 'e', 22: 'f', 23: 'g', 24: 'h', 25: 'i', 26: 'j', 27: 'k', 28: 'l', 29: 'm', 30: 'n', 31: 'o', 32: 'p', 33: 'q', 34: 'r', 35: 's', 36: 't', 37: 'u', 38: 'v', 39: 'w', 40: 'x', 41: 'y', 42: 'z', 43: '‘', 44: '’', 45: '“', 46: '”'}\n"
     ]
    }
   ],
   "source": [
    "int_to_char=dict((i,c)for i,c in enumerate(chars))\n",
    "print(\"Reverse mapping :\\n\",int_to_char)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use seed sequence as input, generate the next character then update the seed sequence to add the generated character on the end and trim of the first character. This process is repeated for as long as we want to predict new characters(e.g. a sequence of 1000 chars in len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed\n",
      "\" the others looked round also, and\n",
      "all of them bowed low.\n",
      "\n",
      "‘would you tell me,’ said alice, a little  \"\n",
      "semiiee, ‘in you dnn the mort brtiors ’huh the gortt ’ou tnonk!’ she said to herself, ‘io was a little so the tooe tf the was shinking the had soe the was oo the thieg aearge, and she tein soe toine an the cade  ‘ht was a little oo the tian whth al c lo ent teaee teat i saou to the thet t thoug the gurhhos weie the sas of the goose  she had not the when tae if the had sote the was soinki  she was soine on the bank, and she tai oo the whrte the was aoo ohe thee, and she toine whet she was ani goren thet sae if the gorrt wo the korke taate \n",
      "the was sorilg and the toeds wfse tuiet in a loetter to ceoen ti the white rabbit. and the wordd had no toem a art oe thet would be armlers  she houst whin she was an the cate tai in a goeat hurry. \n",
      "‘the mort mite the more ’f think,’ said the monke,\n",
      "\n",
      "‘ie horst shin t ali the duchess and bli the tai if then iad sae the was shing the had soee the was oo the thilg  she was soinkigng to the toeee of the table, and she thing the was soinking at the could s\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#pick a random seed\n",
    "start=numpy.random.randint(0,len(dataX)-1)\n",
    "pattern=dataX[start]\n",
    "print(\"Seed\")\n",
    "print(\"\\\"\",''.join([int_to_char[value] for value in pattern]),\"\\\"\")\n",
    "#generate characters\n",
    "\n",
    "for i in range(1000):\n",
    "    x=numpy.reshape(pattern,(1,len(pattern),1))\n",
    "    x=x/float(n_vocab)\n",
    "    prediction=model.predict(x,verbose=0)\n",
    "    index=numpy.argmax(prediction)\n",
    "    result=int_to_char[index]\n",
    "    seq_in=[int_to_char[value] for value in pattern]\n",
    "   # print(\"\\n\\n\\n\\ Generated text: \\n\")\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern=pattern[1:len(pattern)]\n",
    "print(\"\\nDone\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------end-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larger LSTM Recurrent Neural Network\n",
    "\n",
    "We can improve the results or the quality of the generated text by creating a much larger network.\n",
    "\n",
    "---code---\n",
    "-Begins--\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "---end---\n",
    "\n",
    "We will also need to retrain the model and change the file path name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of larger Lstm network will require more resources.\n",
    "Finally, we will increase the number of training epochs from 20 to 50 and decrease the batch size from 128 to 64 to give the network more of an opportunity to be updated and learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
